from cv2 import cv2
import os
from tkinter import *
window=Tk()
window.title("Attendance System")
message = Label(window, text="Attendance-Management-System" ,bg="Green"  ,fg="white"  ,width=50  ,height=3,font=('times', 30, 'italic bold underline')) 
message.place(x=200, y=20)
lbl =Label(window, text="Enter ID",width=20  ,height=2  ,fg="red"  ,bg="yellow" ,font=('times', 15, ' bold ') ) 
lbl.place(x=400, y=200)
txt = Entry(window,width=20  ,bg="yellow" ,fg="red",font=('times', 15, ' bold '))
txt.place(x=700, y=215)
lbl2 = Label(window, text="Enter Name",width=20  ,fg="red"  ,bg="yellow"    ,height=2 ,font=('times', 15, ' bold ')) 
lbl2.place(x=400, y=300)
txt2 =Entry(window,width=20  ,bg="yellow"  ,fg="red",font=('times', 15, ' bold ')  )
txt2.place(x=700, y=315)
def dataset():
    Id=(txt.get())
    name=(txt2.get())
    face_classifier=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
    #file_name=input("Enter name:")
    path='/Users/usha/Desktop/project/data/'
    i=0
    for files in os.walk(path):
                file_name = '{}'.format(i)
                file=os.path.join(path,file_name)
                if not os.path.exists(file):
                     os.mkdir(file)
                else:
                    i=i+1
    def face_crop(img):
        gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
        faces=face_classifier.detectMultiScale(gray,1.3,5)
        if faces is():
           return None
        for(x,y,w,h) in faces:
            offset=5
            cropped_face=frame[y-offset:y+h+offset,x-offset:x+w+offset]
        return cropped_face
    cap=cv2.VideoCapture(0)
    img_id=0
 
    while(True):
        ret,frame=cap.read()
        if face_crop(frame) is not None:
            img_id+=1
            face=cv2.resize(face_crop(frame),(500,500))
            face=cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)
            #file_path='file/'+str(img_id)+".jpg"
            #file_path="data/"+file_name+"_"+str(img_id)+".jpg"
            #file_path=os.path.join(file,"{}.jpg").format(str(img_id))
            file_path=os.path.join(file,"{}{}.jpg").format(name,str(img_id))
            cv2.imwrite(file_path,face)
            cv2.putText(face,str(img_id),(50,50),cv2.FONT_HERSHEY_COMPLEX,1,(0,255,0),2)
            cv2.imshow("Cropped face",face)
            if cv2.waitKey(1)==13 or int(img_id)==60:
                break
    cap.release()
    cv2.destroyAllWindows()
btn1=Button(window, text="New Attendee" ,command=dataset, fg="red"  ,bg="yellow"  ,width=20  ,height=2 ,activebackground = "Red" ,font=('times', 15, ' bold '))
btn1.place(x=400, y=400)
btn2=Button(window, text="Train Dataset" ,fg="red"  ,bg="yellow"  ,width=20  ,height=2 ,activebackground = "Red" ,font=('times', 15, ' bold '))
btn2.place(x=700, y=400)
btn3=Button(window, text="Mark attendance" ,fg="red"  ,bg="yellow"  ,width=20  ,height=2 ,activebackground = "Red" ,font=('times', 15, ' bold '))
btn3.place(x=1000, y=400)
window.mainloop()




training.py

import os
from cv2 import cv2
import numpy as np
from PIL import Image


recognizer = cv2.face.LBPHFaceRecognizer_create() # instantiate the lbph recognizer


path = '/Users/usha/Desktop/project/data/' #setting out path of folders with images

file='/Users/usha/Desktop/project/data/recognizer'
if not os.path.exists(file):
    os.makedirs(file) # making a directory for yml file which will be generated after training

def getImageswithId(path):
    faces = []
    faceid = []

    for root,directory,filenames in os.walk(path):
        for filename in filenames:
            id = os.path.basename(root) #this directly assigns folder name ie 0,1..
            img_path = os.path.join(root,filename)
            print('img_path:',img_path)
            print('id:',id)
            test_img = cv2.imread(img_path)
           
            # test_img = np.float32(test_img)
            if test_img is None:
                print('image not loaded poperly - cv2 cant read!!')
                continue
            #if images in dataset are not in gray scale then use below 3 line

            gray_img=cv2.cvtColor(test_img,cv2.COLOR_BGR2GRAY)#convert color image to grayscale
            face_haar_cascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')#Load haar classifier
            face=face_haar_cascade.detectMultiScale(gray_img,scaleFactor=1.32,minNeighbors=5)#detectMultiScale returns rectangles
           
            if len(face)!=1:
                continue # since we are asuuming only single person images are being fed to classifier

            (x,y,w,h) = face[0]
            gray = gray_img[y:y+h,x:x+h]
            equ = cv2.equalizeHist(gray) 
            final = cv2.medianBlur(equ, 3)
            faces.append(final)
            faceid.append(int(id))

    return faceid,faces


faceid , faces = getImageswithId(path)
recognizer.train(faces,np.array(faceid))
recognizer.save('/Users/usha/Desktop/project/data/recognizer/trainingData.yml')
cv2.destroyAllWindows()























#https://towardsdatascience.com/face-recognition-how-lbph-works-90ec258c3d6b